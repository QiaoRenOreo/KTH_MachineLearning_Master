{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD2421 Machine Learning Lab2: Support Vector Machines\n",
    "\n",
    "import numpy as np\n",
    "import random, math\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate Test Data\n",
    "numpy.random.seed(100) #this gives us the same random data  every time, will help debugging\n",
    "classA = numpy.concatenate((\n",
    "    numpy.random.randn(10, 2) * 0.3 + [1.5, 0.5],\n",
    "    numpy.random.randn(5, 2) * 0.3 + [-1.5, 0.5],\n",
    "    numpy.random.randn(5, 2) * 0.7 + [0.0, -1.0],\n",
    "    ))\n",
    "classB=numpy.random.randn(20, 2) * 0.3 + [0.0, -0.5]\n",
    "\n",
    "inputs=numpy.concatenate((classA, classB))\n",
    "targets=numpy.concatenate((numpy.ones(classA.shape[0]), -numpy.ones(classB.shape[0])))\n",
    "\n",
    "N = inputs.shape[0] # Number of rows (samples)\n",
    "\n",
    "\n",
    "#these four lines randomly reorders the samples, by changing this spread we can move the clusters around\n",
    "permute = list(range(N))\n",
    "random.shuffle(permute)\n",
    "inputs = inputs[permute,:]\n",
    "targets = targets[permute]\n",
    "\n",
    "#some Global Variables\n",
    "KernelMode = 'linear' #poly #RBF #suitable kernel function\n",
    "pol_degree = 2\n",
    "sigma = 12 #OBS I JUST CHOOSE SOME SHITTY VALUE FIX THIS BEFORE TESTING\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kernel_function(x,y):\n",
    "    #pol_degree is the exponent for the polynomial kernel, is a global variable set outside of the function\n",
    "    #sigma controls the smoothness of the boundary for Radial Basis Function Kernel, is a global variable set outside of the function\n",
    "    if KernelMode == 'linear': # for the linear kernel\n",
    "        K = np.dot(x,y)\n",
    "        return K\n",
    "    elif KernelMode == 'poly':\n",
    "        K=np.power(np.dot(x,y)+1,pol_degree)\n",
    "        return K\n",
    "    elif KernelMode =='RBF':\n",
    "        numerator=np.power(math.fabs(np.subtract(x,y)),2)\n",
    "        K = math.exp(-numerator/(2*np.power(sigma,2)))\n",
    "        return K\n",
    "    else:\n",
    "        print('Error: Kernel Mode not valid')\n",
    "        return\n",
    "\n",
    "# precomputed matrix P function\n",
    "def pre_computation(target, input):\n",
    "    P = np.zeros((N, N))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, N):\n",
    "            P[i][j] = (target[i])* (target[j])* kernel_function(input[i], input[j]) #Pi,j = ti*tj*K(Xi, Xj)\n",
    "    return P\n",
    "\n",
    "#Implementation of the function objective which implements equation (4), the function only receives the alpha vector as a parameter\n",
    "def objective(alpha):\n",
    "    return (1/2)*numpy.dot(alpha, numpy.dot(alpha, Pmatrix)) - numpy.sum(alpha)\n",
    "\n",
    "def zerofun(alpha):\n",
    "    return numpy.dot(alpha, targets)\n",
    "\n",
    "def main():\n",
    "\n",
    "    Pmatrix = pre_computation(targets, inputs)\n",
    "    start = numpy.zeros(N)  # N is the number of training samples\n",
    "    C = 10000\n",
    "    XC = {'type': 'eq', 'fun': zerofun}\n",
    "    B = [(0, C) for b in range(N)\n",
    "    ret = minimize(objective, start, bounds=B, constraints=XC)\n",
    "    if (not ret[\n",
    "        'success']):  # The string 'success' instead holds a boolean representing if the optimizer has found a solution\n",
    "        raise ValueError('Cannot find optimizing solution')\n",
    "    # Extract non-zero alphas\n",
    "    alpha = ret['x']\n",
    "    nonzeroalpha = [(alpha[i], inputs[i], targets[i]) for i in range(N) if abs(alpha[i]) > 10e-5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#zerofun function\n",
    "\n",
    "# call minimize\n",
    "\n",
    "#extract the non-zero alpha values\n",
    "\n",
    "# calculate the b value using equation 7\n",
    "\n",
    "# generate test data\n",
    "\n",
    "# code that I Thought was given by the instructions. Unclear usage so far!\n",
    "#start = numpy.zeros(N)\n",
    "#bounds = [(0,C) for b in range(N)]\n",
    "#constraint = {'type':'eq', 'fun':zerofun}\n",
    "#the heart of the program will be single call to the minimize function, looking like this:\n",
    "#ret = minimize(objective, start, bounds = B, constraints = XC)\n",
    "#alpha = ret['x']\n",
    "\n",
    "#plotting the data, lines provided in the lab\n",
    "def plot_data(classA, classB):\n",
    "    plt.plot([p[0] for p in classA], [p[1] for p in classA], 'b.')\n",
    "    plt.plot([p[0] for p in classB], [p[1] for p in classB], 'r.')\n",
    "    plt.axis('equal')  # force same scale on both axes\n",
    "    plt.savefig('svmplot.pdf')  # Save e copy in a file\n",
    "    plt.show()  # show the plot on the screen\n",
    "\n",
    "#plotting the Decision Boundary\n",
    "#xgrid = numpy.linspace(-5,5)\n",
    "#ygrid = numpy.linspace(-4,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
